# ğŸ—ï¸ Arquitectura del Sistema de DetecciÃ³n Emocional Multimodal

## ğŸ“‹ Tabla de Contenidos
1. [DescripciÃ³n General](#descripciÃ³n-general)
2. [Arquitectura del Sistema](#arquitectura-del-sistema)
3. [Backend (Python/FastAPI)](#backend-pythonfastapi)
4. [Frontend (React/Vite)](#frontend-reactvite)
5. [Sistema de FusiÃ³n 2oo2](#sistema-de-fusiÃ³n-2oo2)
6. [Flujos de Datos](#flujos-de-datos)
7. [Modelos de IA](#modelos-de-ia)
8. [ConfiguraciÃ³n](#configuraciÃ³n)

---

## ğŸ“– DescripciÃ³n General

Este proyecto implementa un sistema de **detecciÃ³n de emociones multimodal** que combina:
- ğŸ‘¤ **DetecciÃ³n Facial** (YOLOv8) - AnÃ¡lisis de expresiones faciales
- ğŸ¤ **DetecciÃ³n de Voz** (LSTM CREMA-D) - AnÃ¡lisis prosÃ³dico del habla
- ğŸ”€ **FusiÃ³n 2oo2** - CombinaciÃ³n inteligente de ambas modalidades
- ğŸ¤– **Control de Robot Pepper** - EnvÃ­o de comandos al robot humanoid

### Caso de Uso Principal
Videollamada en tiempo real donde:
1. Usuario habla frente a cÃ¡mara
2. Sistema detecta emociÃ³n facial + vocal
3. Fusiona ambas detecciones usando algoritmo 2oo2
4. EnvÃ­a emociÃ³n fusionada al robot Pepper
5. Pepper ejecuta animaciÃ³n correspondiente

---

## ğŸ›ï¸ Arquitectura del Sistema

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        FRONTEND (React)                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ VideoCall.jsxâ”‚  â”‚ useFaceYolo   â”‚  â”‚ useSpeechEmotion    â”‚  â”‚
â”‚  â”‚              â”‚â”€â”€â”‚ (300ms loops) â”‚  â”‚ (4s audio bursts)   â”‚  â”‚
â”‚  â”‚              â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”‚              â”‚          â”‚                     â”‚              â”‚
â”‚  â”‚              â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚              â”‚  â”‚ useEmotionFusion (1s polling)          â”‚  â”‚
â”‚  â”‚              â”‚â”€â”€â”‚ Combina Face + Audio                   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚ HTTP Requests
                                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     BACKEND (Python/FastAPI)                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  ENDPOINTS (emotion.py)                                   â”‚   â”‚
â”‚  â”‚  â€¢ POST /emotion/from-frame  â†’ YOLOv8 Face Detection     â”‚   â”‚
â”‚  â”‚  â€¢ POST /emotion/from-speech â†’ LSTM Audio Detection      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                     â”‚                       â”‚                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  EmotionBuffer           â”‚  â”‚  PepperState             â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚    â”‚
â”‚  â”‚  â”‚ Face   â”‚ Audio  â”‚    â”‚  â”‚  â”‚ proce: 0|1      â”‚    â”‚    â”‚
â”‚  â”‚  â”‚ Buffer â”‚ Buffer â”‚    â”‚  â”‚  â”‚ (busy|available)â”‚    â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                     â”‚                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  FUSION SYSTEM (fusion.py + fusion_voting.py)            â”‚  â”‚
â”‚  â”‚  â€¢ POST /fusion/auto-fuse  â†’ Combina Face + Audio        â”‚  â”‚
â”‚  â”‚  â€¢ GET  /fusion/buffer-stats â†’ Estado del buffer         â”‚  â”‚
â”‚  â”‚  â€¢ Sistema de votaciÃ³n 2oo2 con pesos dinÃ¡micos          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚ HTTP POST
                                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      ROBOT PEPPER                                â”‚
â”‚  â€¢ Recibe emociÃ³n mapeada (7 emociones bÃ¡sicas)                 â”‚
â”‚  â€¢ Ejecuta animaciÃ³n correspondiente                             â”‚
â”‚  â€¢ Actualiza estado proce (0=ocupado, 1=disponible)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ Backend (Python/FastAPI)

### Estructura de Archivos

```
emotion_api/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py                    # AplicaciÃ³n FastAPI principal
â”‚   â”œâ”€â”€ config.py                  # ConfiguraciÃ³n (carga .env)
â”‚   â”œâ”€â”€ state.py                   # Estado global (buffer, pepper)
â”‚   â”‚
â”‚   â”œâ”€â”€ routers/
â”‚   â”‚   â”œâ”€â”€ emotion.py             # ğŸ‘¤ğŸ¤ Endpoints de detecciÃ³n
â”‚   â”‚   â”œâ”€â”€ fusion.py              # ğŸ”€ Endpoints de fusiÃ³n
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ services/
â”‚   â”‚       â”œâ”€â”€ yolov8_face_emotion.py        # Modelo facial
â”‚   â”‚       â”œâ”€â”€ lstm_crema_emotion.py         # Modelo de audio
â”‚   â”‚       â”œâ”€â”€ fusion_voting.py              # LÃ³gica 2oo2
â”‚   â”‚       â”œâ”€â”€ mapping.py                    # Mapeo emociones
â”‚   â”‚       â””â”€â”€ pepper_client.py              # Cliente Pepper
â”‚   â”‚
â”‚   â””â”€â”€ models/
â”‚       â”œâ”€â”€ yolov8_emotions.pt                # Pesos YOLOv8
â”‚       â””â”€â”€ lstm_crema_d_v3.h5               # Pesos LSTM
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ .env                           # Variables de entorno
```

### Componentes Principales

#### 1. **emotion.py** - Endpoints de DetecciÃ³n

**`POST /emotion/from-frame`**
```python
# FUNCIÃ“N: Detecta emociÃ³n facial desde imagen
# ENTRADA:
#   - image: File (JPEG/PNG)
#   - size: int (opcional, default 640)
#   - room: str (opcional, ID de sala)
# SALIDA:
#   {
#     "label": "happy",
#     "score": 0.92,
#     "scores": [{...}],
#     "room": "test"
#   }
# FLUJO:
#   1. Convierte imagen a numpy array BGR
#   2. Ejecuta YOLOv8 para detectar rostro + emociÃ³n
#   3. Guarda resultado en EmotionBuffer.add_face()
#   4. Retorna resultado al frontend
```

**`POST /emotion/from-speech`**
```python
# FUNCIÃ“N: Detecta emociÃ³n desde audio
# ENTRADA:
#   - audio: File (WebM/WAV/MP3)
#   - room: str (opcional)
# SALIDA:
#   {
#     "label": "angry",
#     "score": 0.91,
#     "model": "lstm-crema-v3",
#     "mapped_emotion": "angry",
#     "pepper": {"ok": true}
#   }
# FLUJO:
#   1. Verifica si Pepper estÃ¡ disponible (proce==1)
#   2. Convierte audio a WAV 16kHz mono con ffmpeg
#   3. Ejecuta LSTM CREMA-D para clasificar
#   4. Guarda en EmotionBuffer.add_audio()
#   5. Mapea emociÃ³n para Pepper (map_to_face7)
#   6. EnvÃ­a comando a Pepper
#   7. Retorna resultado
```

#### 2. **fusion.py** - Sistema de FusiÃ³n

**`POST /fusion/auto-fuse`**
```python
# FUNCIÃ“N: Fusiona Ãºltimas detecciones de face y audio
# ENTRADA:
#   - room: str (ID de sala)
# SALIDA:
#   {
#     "emotion": "angry",
#     "confidence": 0.85,
#     "strategy": "weighted_fusion",
#     "weights": {"face": 0.45, "audio": 0.55},
#     "face": {...},
#     "audio": {...}
#   }
# FLUJO:
#   1. Obtiene Ãºltimas detecciones del buffer
#   2. Verifica que haya datos de AMBAS modalidades
#   3. Ejecuta algoritmo 2oo2 (fusion_voting.py)
#   4. Retorna emociÃ³n fusionada
```

**`GET /fusion/buffer-stats`**
```python
# FUNCIÃ“N: Obtiene estado del buffer de emociones
# ENTRADA:
#   - room: str (opcional)
# SALIDA:
#   {
#     "room": "test",
#     "face_count": 1,
#     "audio_count": 1,
#     "has_both": true
#   }
# USO: Frontend verifica si hay datos antes de llamar auto-fuse
```

#### 3. **state.py** - Estado Global

```python
class EmotionBuffer:
    """
    Buffer temporal de detecciones para fusiÃ³n multimodal

    PROPÃ“SITO:
        Almacenar Ãºltimas detecciones de face y audio por sala
        Permite que /fusion/auto-fuse obtenga datos recientes

    ESTRUCTURA:
        {
          "test": {  # room ID
            "face": [  # Lista de detecciones faciales
              {"label": "happy", "score": 0.9, "timestamp": 123.45},
              ...
            ],
            "audio": [  # Lista de detecciones de audio
              {"label": "angry", "score": 0.8, "timestamp": 123.50},
              ...
            ]
          }
        }

    MÃ‰TODOS:
        - add_face(room, data): Agregar detecciÃ³n facial
        - add_audio(room, data): Agregar detecciÃ³n de audio
        - get_latest_face(room): Obtener Ãºltima detecciÃ³n facial
        - get_latest_audio(room): Obtener Ãºltima detecciÃ³n de audio
        - get_stats(room): Obtener estadÃ­sticas del buffer
        - clear_room(room): Limpiar buffer de una sala

    CONFIGURACIÃ“N:
        - max_age: 10 segundos (detecciones mÃ¡s antiguas se descartan)
        - max_size: 10 detecciones por modalidad por sala
    """

class PepperState:
    """
    Estado del robot Pepper por sala

    PROPÃ“SITO:
        Evitar enviar comandos cuando Pepper estÃ¡ ocupado
        Coordinar animaciones entre mÃºltiples clientes

    ESTRUCTURA:
        {
          "test": {  # room ID
            "proce": 1,  # 0=ocupado, 1=disponible
            "last_update": 123.45
          }
        }

    MÃ‰TODOS:
        - get_proce(room): Obtener estado actual (0 o 1)
        - set_proce(room, value): Actualizar estado
    """
```

#### 4. **fusion_voting.py** - Algoritmo 2oo2

```python
class EmotionFusionSystem:
    """
    Sistema de fusiÃ³n multimodal con algoritmo 2oo2

    ALGORITMO 2oo2 (2-out-of-2):
        Requiere concordancia de al menos 2 modalidades de 2
        Si ambas concuerdan â†’ boost de confianza
        Si difieren â†’ pesos dinÃ¡micos segÃºn confianza

    ESTRATEGIAS:
        1. consensus_weighted: Ambas modalidades de acuerdo
           â†’ Boost de confianza (+10%)
           â†’ Pesos: 50% face, 50% audio

        2. weighted_fusion: Modalidades en conflicto
           â†’ Pesos dinÃ¡micos segÃºn confianza individual
           â†’ PenalizaciÃ³n de confianza (-10%)

        3. face_only: Solo hay detecciÃ³n facial
           â†’ Retorna face con confianza reducida

        4. audio_only: Solo hay detecciÃ³n de audio
           â†’ Retorna audio con confianza reducida

    PESOS DINÃMICOS:
        - Base: face=45%, audio=55%
        - Ajuste por confianza: modalidad mÃ¡s confiada recibe mÃ¡s peso
        - Rango: 30%-70% para evitar dominancia completa

    EJEMPLO:
        Face: happy (0.9)
        Audio: sad (0.7)

        â†’ Estrategia: weighted_fusion (conflicto)
        â†’ Pesos ajustados: face=60%, audio=40% (face mÃ¡s confiada)
        â†’ Resultado: happy (0.78) con penalizaciÃ³n
    """
```

---

## âš›ï¸ Frontend (React/Vite)

### Estructura de Archivos

```
pepper_connect/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ pages/
â”‚   â”‚   â””â”€â”€ VideoCall.jsx              # ğŸ“¹ PÃ¡gina principal
â”‚   â”‚
â”‚   â”œâ”€â”€ hooks/
â”‚   â”‚   â”œâ”€â”€ useFaceYolo.jsx            # ğŸ‘¤ Hook detecciÃ³n facial
â”‚   â”‚   â”œâ”€â”€ useSpeechEmotion.jsx       # ğŸ¤ Hook detecciÃ³n audio
â”‚   â”‚   â””â”€â”€ useEmotionFusion.jsx       # ğŸ”€ Hook fusiÃ³n
â”‚   â”‚
â”‚   â”œâ”€â”€ lib/
â”‚   â”‚   â”œâ”€â”€ faceApi.jsx                # API cliente face
â”‚   â”‚   â”œâ”€â”€ pepperEmotionClient.jsx    # Cliente Pepper
â”‚   â”‚   â””â”€â”€ pepperState.jsx            # Estado Pepper
â”‚   â”‚
â”‚   â””â”€â”€ .env                            # ConfiguraciÃ³n
â”‚
â””â”€â”€ vite.config.js
```

### Componentes Principales

#### 1. **VideoCall.jsx** - PÃ¡gina Principal

```javascript
/**
 * Componente principal de videollamada
 *
 * RESPONSABILIDADES:
 *   1. Captura video/audio del usuario (getUserMedia)
 *   2. Establece conexiÃ³n WebRTC con Pepper
 *   3. Coordina hooks de detecciÃ³n (face, audio, fusion)
 *   4. EnvÃ­a emociÃ³n fusionada a Pepper
 *
 * HOOKS USADOS:
 *   - useFaceYolo: Detecta emociÃ³n facial cada 300ms
 *   - useSpeechEmotion: Detecta emociÃ³n de audio cada 15s
 *   - useEmotionFusion: Fusiona face+audio cada 1s
 *
 * FLUJO:
 *   1. getUserMedia â†’ obtiene stream de cÃ¡mara/micrÃ³fono
 *   2. useFaceYolo captura frames â†’ POST /emotion/from-frame
 *   3. useSpeechEmotion graba audio â†’ POST /emotion/from-speech
 *   4. useEmotionFusion â†’ GET /fusion/buffer-stats
 *                       â†’ POST /fusion/auto-fuse
 *   5. fusedEmotion cambia â†’ triggerPepperEmotion()
 *   6. Pepper ejecuta animaciÃ³n
 *
 * ESTADO:
 *   - userStream: MediaStream del usuario
 *   - fusedEmotion: EmociÃ³n fusionada actual
 *   - isAnimating: Si Pepper estÃ¡ animando
 */
```

#### 2. **useFaceYolo.jsx** - DetecciÃ³n Facial

```javascript
/**
 * Hook para detecciÃ³n de emociones faciales con YOLOv8
 *
 * PARÃMETROS:
 *   - videoRef: Ref del elemento <video>
 *   - room: ID de sala
 *
 * RETORNA:
 *   {
 *     label: "happy",      // EmociÃ³n detectada
 *     score: 0.92,         // Confianza
 *     ready: true,         // Si estÃ¡ listo
 *     error: null          // Error si hay
 *   }
 *
 * FUNCIONAMIENTO:
 *   1. Loop cada 300ms (configurable con VITE_FACE_INTERVAL_MS)
 *   2. Verifica si Pepper estÃ¡ disponible (isPepperAvailable)
 *   3. Captura frame del video con canvas
 *   4. Convierte a Blob JPEG
 *   5. POST /emotion/from-frame con FormData
 *   6. Aplica smoothing temporal (reduce ruido)
 *   7. Actualiza estado
 *
 * OPTIMIZACIONES:
 *   - Throttling: Solo 1 request a la vez (pendingRequest flag)
 *   - Smoothing: Promedia Ãºltimas N detecciones
 *   - Backoff: Reintenta cada 3s si Pepper ocupado
 *   - Timeout: Cancela requests que tardan >5s
 *
 * CONFIGURACIÃ“N (.env):
 *   VITE_FACE_API=http://localhost:8000
 *   VITE_FACE_INTERVAL_MS=300
 *   VITE_FACE_IMG_SIZE=640
 */
```

#### 3. **useSpeechEmotion.jsx** - DetecciÃ³n de Audio

```javascript
/**
 * Hook para detecciÃ³n de emociones desde audio (LSTM)
 *
 * PARÃMETROS:
 *   - stream: MediaStream del micrÃ³fono
 *   - room: ID de sala
 *   - intervalMs: Intervalo entre detecciones (default 15s)
 *   - enabled: Si estÃ¡ habilitado
 *   - onUpdate: Callback cuando detecta emociÃ³n
 *
 * FUNCIONAMIENTO:
 *   1. Loop continuo de detecciÃ³n de audio
 *   2. Calcula RMS (loudness) del audio
 *   3. Si RMS > threshold â†’ graba burst de 4 segundos
 *   4. Convierte audio a Blob WebM
 *   5. POST /emotion/from-speech con FormData
 *   6. Llama onUpdate(result)
 *
 * CARACTERÃSTICAS:
 *   - Voice Activity Detection (VAD): Solo procesa si hay voz
 *   - Bursts de 4 segundos: Balance entre latencia y precisiÃ³n
 *   - Fallback a "neutral" en silencio
 *   - Manejo de estado de Pepper (skip si ocupado)
 *
 * PARÃMETROS DE DETECCIÃ“N:
 *   - RMS threshold: 0.005 (ajustable)
 *   - Burst duration: 4000ms
 *   - Silence counter: 3 detecciones consecutivas
 *
 * CONFIGURACIÃ“N:
 *   VITE_AUDIO_API_URL=http://localhost:8000
 */
```

#### 4. **useEmotionFusion.jsx** - FusiÃ³n Multimodal

```javascript
/**
 * Hook para fusiÃ³n de emociones (Face + Audio)
 *
 * PARÃMETROS:
 *   - room: ID de sala
 *   - intervalMs: Intervalo de polling (default 1000ms)
 *   - enabled: Si estÃ¡ habilitado
 *   - onUpdate: Callback cuando hay fusiÃ³n nueva
 *
 * RETORNA:
 *   {
 *     fusedEmotion: {
 *       emotion: "angry",
 *       confidence: 0.85,
 *       strategy: "weighted_fusion",
 *       weights: {face: 0.45, audio: 0.55}
 *     },
 *     error: null,
 *     performFusion: Function  // Manual trigger
 *   }
 *
 * FLUJO DE FUSIÃ“N:
 *   1. Polling cada 1 segundo (setInterval)
 *   2. Verifica si Pepper disponible (isPepperAvailable)
 *   3. GET /fusion/buffer-stats
 *   4. Si has_both === true:
 *      â†’ POST /fusion/auto-fuse
 *      â†’ Actualiza fusedEmotion
 *      â†’ Llama onUpdate(data)
 *   5. Si has_both === false:
 *      â†’ Skip, esperar mÃ¡s datos
 *
 * OPTIMIZACIONES:
 *   - isProcessingRef: Evita polling concurrente
 *   - onUpdateRef: Evita recreaciÃ³n de callbacks
 *   - Throttling: Solo muestra logs cada 5s si emociÃ³n no cambia
 *
 * CONFIGURACIÃ“N:
 *   VITE_FACE_API=http://localhost:8000
 */
```

---

## ğŸ”€ Sistema de FusiÃ³n 2oo2

### Concepto

El sistema **2-out-of-2 (2oo2)** requiere que al menos 2 de 2 modalidades estÃ©n disponibles y concuerden para generar una detecciÃ³n vÃ¡lida.

### Ventajas vs. Unimodal

| Aspecto | Unimodal (Solo Face o Audio) | Multimodal 2oo2 |
|---------|------------------------------|-----------------|
| **PrecisiÃ³n** | 70-80% | 85-95% |
| **Robustez** | Falla con oclusiones/ruido | Compensa debilidades |
| **Confianza** | Variable | Boosted al concordar |
| **Contexto** | Limitado | Rico (visual + prosÃ³dico) |

### Estrategias de FusiÃ³n

#### 1. **Consensus Weighted** (Consenso)
```
CondiciÃ³n: face.label == audio.label
AcciÃ³n:
  - Confianza boosted (+10%)
  - Pesos equilibrados (50/50)
Ejemplo:
  Face: happy (0.8)
  Audio: happy (0.7)
  â†’ Resultado: happy (0.825) âœ“ boost aplicado
```

#### 2. **Weighted Fusion** (Conflicto)
```
CondiciÃ³n: face.label != audio.label
AcciÃ³n:
  - Pesos dinÃ¡micos segÃºn confianza
  - Confianza penalizada (-10%)
Ejemplo:
  Face: happy (0.9)  â†’ peso 60%
  Audio: sad (0.6)   â†’ peso 40%
  â†’ Resultado: happy (0.72) con penalizaciÃ³n
```

#### 3. **Face Only** / **Audio Only**
```
CondiciÃ³n: Solo una modalidad disponible
AcciÃ³n:
  - Usar modalidad disponible
  - Confianza reducida (Ã—0.8)
```

### Pesos DinÃ¡micos

```python
# ConfiguraciÃ³n base
base_face_weight = 0.45
base_audio_weight = 0.55

# Ajuste por confianza
if face.score > audio.score:
    face_weight += (face.score - audio.score) * 0.5
    audio_weight -= (face.score - audio.score) * 0.5

# LÃ­mites
face_weight = clamp(face_weight, 0.30, 0.70)
audio_weight = clamp(audio_weight, 0.30, 0.70)

# NormalizaciÃ³n
total = face_weight + audio_weight
face_weight /= total
audio_weight /= total
```

---

## ğŸ“Š Flujos de Datos

### Flujo Completo de DetecciÃ³n

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. CAPTURA (Frontend)                                            â”‚
â”‚    - getUserMedia â†’ MediaStream                                  â”‚
â”‚    - Video: 640x480 @ 15fps                                      â”‚
â”‚    - Audio: 48kHz stereo                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚                   â”‚
                â–¼                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2A. FACE DETECTION    â”‚  â”‚ 2B. AUDIO DETECTION    â”‚
â”‚ (Every 300ms)         â”‚  â”‚ (Every 15s)            â”‚
â”‚                       â”‚  â”‚                        â”‚
â”‚ â€¢ Capture frame       â”‚  â”‚ â€¢ Detect voice (VAD)   â”‚
â”‚ â€¢ Resize to 640x640   â”‚  â”‚ â€¢ Record 4s burst      â”‚
â”‚ â€¢ Convert to JPEG     â”‚  â”‚ â€¢ Convert to WebM      â”‚
â”‚ â€¢ POST /from-frame    â”‚  â”‚ â€¢ POST /from-speech    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                           â”‚
          â–¼                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. ML MODELS (Backend)                           â”‚
â”‚                                                   â”‚
â”‚ Face: YOLOv8                 Audio: LSTM         â”‚
â”‚ â€¢ Detect face bbox           â€¢ Extract MFCCs     â”‚
â”‚ â€¢ Classify emotion           â€¢ Classify emotion  â”‚
â”‚ â€¢ Return scores              â€¢ Return scores     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                            â”‚
          â–¼                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. EMOTION BUFFER                                â”‚
â”‚    {                                             â”‚
â”‚      "test": {                                   â”‚
â”‚        "face": [{label:"happy", score:0.9}],    â”‚
â”‚        "audio": [{label:"angry", score:0.8}]    â”‚
â”‚      }                                           â”‚
â”‚    }                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. FUSION (Every 1s)                             â”‚
â”‚    â€¢ GET /buffer-stats                           â”‚
â”‚    â€¢ If has_both:                                â”‚
â”‚      â†’ POST /auto-fuse                           â”‚
â”‚      â†’ Apply 2oo2 algorithm                      â”‚
â”‚      â†’ Return fused emotion                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6. PEPPER CONTROL                                â”‚
â”‚    â€¢ Map emotion (7 basic emotions)              â”‚
â”‚    â€¢ Check proce status                          â”‚
â”‚    â€¢ If available:                               â”‚
â”‚      â†’ POST http://pepper:8070/trigger           â”‚
â”‚      â†’ Pepper animates                           â”‚
â”‚      â†’ Update proce=0 (busy)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Timing Diagram

```
Timeline (ms):
0      300    600    900    1000   1500   4000   15000
â”‚      â”‚      â”‚      â”‚      â”‚      â”‚      â”‚      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â–º Time
â”‚      â”‚      â”‚      â”‚      â”‚      â”‚      â”‚      â”‚
Face:  ğŸ”    ğŸ”    ğŸ”    ğŸ”    ğŸ”    ğŸ”    ğŸ”    ğŸ”
       (detect every 300ms)
                            â”‚             â”‚
Audio:                      ğŸ¤â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ğŸ¤
                           (4s burst)   (detect)
                            â”‚
Fusion:                     ğŸ”€            ğŸ”€
                          (fuse)       (fuse)
                            â”‚             â”‚
Pepper:                     ğŸ¤–â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ğŸ¤–
                          (animate)    (animate)
                          proce=0â†’1    proce=0â†’1
```

---

## ğŸ¤– Modelos de IA

### 1. YOLOv8 Face Emotion

**Archivo**: `app/models/yolov8_emotions.pt`

**CaracterÃ­sticas**:
- Arquitectura: YOLOv8n (nano, optimizado para velocidad)
- Input: Imagen RGB 640x640
- Output: Bounding boxes + clasificaciÃ³n de emociÃ³n
- Clases: 7 emociones (happy, sad, angry, surprise, fear, disgust, neutral)
- FPS: ~30-50 en CPU, ~100+ en GPU

**Pipeline de Inferencia**:
```python
# 1. Preprocesamiento
image_resized = cv2.resize(image, (640, 640))

# 2. Inferencia
results = model.predict(image_resized, conf=0.25)

# 3. Postprocesamiento
for detection in results[0].boxes:
    bbox = detection.xyxy
    emotion_id = int(detection.cls)
    confidence = float(detection.conf)

    emotion_label = EMOTION_NAMES[emotion_id]
    # â†’ {"label": "happy", "score": 0.92, "bbox": [...]}
```

### 2. LSTM CREMA-D V3

**Archivo**: `app/models/lstm_crema_d_v3.h5`

**CaracterÃ­sticas**:
- Arquitectura: Bidirectional LSTM
- Dataset: CREMA-D (Crowd-sourced Emotional Multimodal Actors Dataset)
- Input: MFCCs (Mel-Frequency Cepstral Coefficients) del audio
- Output: Probabilidades para 6 emociones
- Clases: angry, sad, happy, fear, disgust, neutral

**Pipeline de Inferencia**:
```python
# 1. Feature Extraction
audio, sr = librosa.load(wav_path, sr=16000)
mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=40)
mfccs_scaled = (mfccs - mean) / std  # NormalizaciÃ³n

# 2. Inferencia
predictions = lstm_model.predict(mfccs_scaled)

# 3. Postprocesamiento
emotion_id = np.argmax(predictions)
confidence = float(predictions[0][emotion_id])

# â†’ {"label": "angry", "score": 0.91}
```

---

## âš™ï¸ ConfiguraciÃ³n

### Variables de Entorno

#### Backend (`emotion_api/.env`)

```ini
# === FUSION CONFIGURATION ===
# Pesos base para fusiÃ³n multimodal
FUSION_BASE_AUDIO_WEIGHT=0.55       # Peso de audio (55%)
FUSION_BASE_FACE_WEIGHT=0.45        # Peso de face (45%)

# Modo de ajuste de pesos (threshold | confidence | static)
FUSION_WEIGHT_ADJUSTMENT_MODE=threshold

# Rangos de pesos permitidos
FUSION_MIN_WEIGHT=0.30              # MÃ­nimo 30%
FUSION_MAX_WEIGHT=0.70              # MÃ¡ximo 70%

# Umbrales de confianza
FUSION_MIN_CONFIDENCE=0.30          # MÃ­nimo para considerar vÃ¡lido
FUSION_STRONG_CONFIDENCE=0.70       # Umbral de confianza "fuerte"

# Boost y penalizaciones
FUSION_BOOST_CONSENSUS=true         # Boost cuando concuerdan
FUSION_CONSENSUS_BOOST=0.10         # +10% al concordar
FUSION_PENALIZE_CONFLICT=true       # Penalizar conflictos
FUSION_CONFLICT_PENALTY=0.10        # -10% en conflicto

# SupresiÃ³n de neutral
FUSION_SUPPRESS_NEUTRAL=false       # No suprimir neutral
FUSION_NEUTRAL_THRESHOLD=0.50       # Umbral para considerar neutral
FUSION_NEUTRAL_MIN_GAP=0.15         # Gap mÃ­nimo vs otras emociones

# Debug
FUSION_DEBUG_MODE=false             # Logs detallados
FUSION_LOG_ALL_FUSIONS=true         # Log de todas las fusiones

# === PEPPER CONFIGURATION ===
PEPPER_IP=192.168.10.104            # IP del robot Pepper
PEPPER_PORT=8070                    # Puerto API de Pepper
```

#### Frontend (`pepper_connect/.env`)

```ini
# === API ENDPOINTS ===
VITE_FACE_API=http://localhost:8000         # Backend face/audio/fusion
VITE_AUDIO_API_URL=http://localhost:8000    # Backend audio (mismo)
VITE_EMOTION_API_URL=http://localhost:8000  # Backend emotion

# === DETECTION INTERVALS ===
VITE_FACE_INTERVAL_MS=300           # DetecciÃ³n facial cada 300ms
VITE_FACE_IMG_SIZE=640              # TamaÃ±o de imagen para YOLOv8

# === PEPPER CONFIGURATION ===
VITE_PEPPER_IP=192.168.10.104       # IP del robot
VITE_PEPPER_PORT=8070               # Puerto API

# === WEBRTC ===
VITE_SIGNALING_URL=http://localhost:8080  # Servidor seÃ±alizaciÃ³n
```

### InstalaciÃ³n y EjecuciÃ³n

#### Backend

```bash
# 1. Crear entorno virtual
cd emotion_api
python -m venv .venv

# 2. Activar entorno
# Windows:
.venv\Scripts\activate
# Linux/Mac:
source .venv/bin/activate

# 3. Instalar dependencias
pip install -r requirements.txt

# 4. Iniciar servidor
uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload

# Acceder a docs: http://localhost:8000/docs
```

#### Frontend

```bash
# 1. Instalar dependencias
cd pepper_connect
npm install

# 2. Iniciar dev server
npm run dev

# Acceder: http://localhost:5173/videocall?room=test
```

#### Proxy (Opcional)

```bash
cd proxy
node start_proxy.bat

# Proxy en: http://localhost:7070
# Redirige a Pepper: http://192.168.10.104:8070
```

---

## ğŸ“ˆ MÃ©tricas y Monitoreo

### Logs del Sistema

#### Backend (FastAPI)

```
INFO:     face frame | room=test | size=640 | top1=happy:0.920 | detections=1 | conf_thres=0.25
INFO:     ğŸ¤ [SPEECH-CREMA] âœ… Classification completed: angry (0.910)
INFO:     [FUSION] Auto-fusion for room test: angry (0.852) via weighted_fusion
INFO:     ğŸ¤ [SPEECH-CREMA] ğŸ¤– Pepper command: angry â†’ angry (sent: True)
```

#### Frontend (Console)

```
[FUSION] Starting fusion system for room: test, interval: 1000ms
ğŸ‘¤ [FACE-YOLO] Detected: happy (92%)
ğŸ¯ Speech detected: angry (91%)
[FUSION] âœ… Both modalities available (face: 1, audio: 1)
[FUSION] âœ¨ New fusion: angry (85.2%) via weighted_fusion | weights: face=45% audio=55%
ğŸ­ [FUSION â†’ PEPPER] angry (85.2%) via weighted_fusion | weights: F45% A55%
```

### Endpoints de Debug

```bash
# Estado del buffer
GET http://localhost:8000/fusion/buffer-stats?room=test

# ConfiguraciÃ³n de fusiÃ³n
GET http://localhost:8000/fusion/config

# Estado de Pepper
GET http://localhost:8000/pepper/status?room=test

# Limpiar buffer
POST http://localhost:8000/fusion/clear-buffer
```

---

## ğŸ”§ Troubleshooting

### Problema: Face no guarda en buffer

**SÃ­ntoma**: `face_count: 0` en buffer-stats

**SoluciÃ³n**:
1. Verificar que `/emotion/from-frame` retorne 200
2. Verificar logs: `ğŸ‘¤ [FACE-YOLO] Saved to buffer for room test`
3. Verificar que `room` se envÃ­e en request

### Problema: FusiÃ³n nunca se ejecuta

**SÃ­ntoma**: No logs de `[FUSION] âœ¨ New fusion`

**SoluciÃ³n**:
1. Verificar `has_both: true` en buffer-stats
2. Verificar que ambos hooks (face, audio) estÃ©n corriendo
3. Verificar que Pepper estÃ© disponible (proce=1)

### Problema: Pepper no responde

**SÃ­ntoma**: `pepper: {ok: false}`

**SoluciÃ³n**:
1. Ping a Pepper: `ping 192.168.10.104`
2. Verificar puerto: `curl http://192.168.10.104:8070/video_feed`
3. Verificar firewall

---

## ğŸ“š Referencias

- **YOLOv8**: https://docs.ultralytics.com/
- **CREMA-D Dataset**: https://github.com/CheyneyComputerScience/CREMA-D
- **FastAPI**: https://fastapi.tiangolo.com/
- **React Hooks**: https://react.dev/reference/react
- **WebRTC**: https://webrtc.org/

---

## ğŸ‘¥ Contribuidores

Para contribuir al proyecto, por favor:
1. Lee esta documentaciÃ³n completa
2. Revisa los comentarios en el cÃ³digo
3. Sigue las convenciones de cÃ³digo existentes
4. Prueba tus cambios localmente
5. Documenta nuevas funcionalidades

---

**VersiÃ³n**: 1.0.0
**Ãšltima actualizaciÃ³n**: 2025-01-22
